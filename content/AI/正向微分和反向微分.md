## 正向微分和反向微分

我们以 $f\left(x_1, x_2\right)=\ln \left(x_1\right)+x_1 x_2-\sin \left(x_2\right)$ 为例子。
计算过程如下：

首先，想象你把它解析成了一个 AST，将所有中间变量编号。

$$
\begin{aligned}
& v_{-1}=x_1 \\
& v_0=x_2 \\
& \hline v_1=\ln v_{-1} \\
& v_2=v_{-1} \times v \\
& v_3=\sin v_0 \\
& v_4=v_1+v_2 \\
& v_5=v_4-v_3 \\
& \\
& y=v_5
\end{aligned}
$$
正向微分很好理解，就是我们传统的高数老师教的方法。

举个例子，$v_{3} = \sin v_{0}$ 的对 $v_{0}$ 正向微分就是 $\dot{v_{3}} = \dot{v_{0}}\cos v_{0}$. （为什么不是 $\dot{v_{3}} = \cos v_{0}$ ? 其实也是的，因为这里 $\dot{v_{0}} = \frac{\partial{v_{0}}}{\partial v_{0}} = 1$）

$$\begin{array}{|c|c|c|}
\hline \begin{array}{l}
\dot{v}_{-1} \\
\dot{v}_0
\end{array} & \begin{array}{l}
=\dot{x}_1 \\
=\dot{x}_2
\end{array} & \begin{array}{l}
=1 \\
=0
\end{array} \\
\hline \dot{v}_1 & =\dot{v}_{-1} / v_{-1} & =1 / 2 \\
\hline \dot{v}_2 & =\dot{v}_{-1} \times v_0+\dot{v}_0 \times v_{-1} & =1 \times 5+0 \times 2 \\
\hline \dot{v}_3 & =\dot{v}_0 \times \cos v_0 & =0 \times \cos 5 \\
\hline \dot{v}_4 & =\dot{v}_1+\dot{v}_2 & =0.5+5 \\
\hline \dot{v}_5 & =\dot{v}_4-\dot{v}_3 & =5.5-0 \\
\hline
\end{array}
$$
全部正向微分完毕之后，我们得到 $\frac{ \partial y }{ \partial x_{1} } = 5.5$.

使用同样的方法可以计算对 $x_{1}$ 的微分。

下面介绍反向微分，它可以同时计算出对 $x_{1}$ 和 $x_{2}$ 的微分。

我们看，$v_{5} = v_{4} - v_{3}$，对它反向微分的话，可以分别以 $v_{4}$ 和 $v_{3}$ 为主体。
- 以 $v_{4}$ 为主体，则 $\frac{ \partial v_{5} }{ \partial v_{5} } = \frac{ \partial v_{4} }{ \partial v_{5} } - 0$，因此 $\bar{v_{4}} = 1$
- 以 $v_{3}$ 为主体，则 $\frac{ \partial v_{5} }{ \partial v_{5} } = 0 - \frac{ \partial v_{3} }{ \partial v_{5} }$，因此 $\bar{v_{3}} = -1$

将这种方法逐个应用，可以从下往上计算出如下结果，和正向微分一样，但充分利用了中间结果，可以同时计算出对 $x_{1}$ 和 $x_{2}$ 的偏导数。

$$
\begin{array}{lll}
\overline{\boldsymbol{x}}_{\mathbf{1}}=\overline{\boldsymbol{v}}_{-\mathbf{1}} & & \mathbf{5 . 5} \\
\overline{\boldsymbol{x}}_{\mathbf{2}}=\overline{\boldsymbol{v}}_{\mathbf{0}} & & \mathbf{1 . 7 1 6} \\
\hline \bar{v}_{-1}=\bar{v}_{-1}+\bar{v}_1 \frac{\partial v_1}{\partial v_{-1}} & =\bar{v}_{-1}+\bar{v}_1 / v_{-1} & =5.5 \\
\bar{v}_0=\bar{v}_0+\bar{v}_2 \frac{\partial v_2}{\partial v_0} & =\bar{v}_0+\bar{v}_2 \times v_{-1} & =1.716 \\
\bar{v}_{-1}=\bar{v}_2 \frac{\partial v_2}{\partial v_{-1}} & =\bar{v}_2 \times v_0 & =5 \\
\bar{v}_0=\bar{v}_3 \frac{\partial v_3}{\partial v_0} & =\bar{v}_3 \times \cos v_0 & =-0.284 \\
\bar{v}_2=\bar{v}_4 \frac{\partial v_4}{\partial v_2} & =\bar{v}_4 \times 1 & =1 \\
\bar{v}_1=\bar{v}_4 \frac{\partial v_4}{\partial v_1} & =\bar{v}_4 \times 1 & =1 \\
\bar{v}_3=\bar{v}_5 \frac{\partial v_5}{\partial v_3} & =\bar{v}_5 \times(-1) & =-1 \\
\bar{v}_4=\bar{v}_5 \frac{\partial v_5}{\partial v_4} & =\bar{v}_5 \times 1 & =1 \\
\hline \bar{v}_5=\bar{y} & =1 &
\end{array}
$$

## Jacobian 矩阵

对于一个机器学习框架，需要处理的不但是一维变量，还需要处理多种维度的张量。

### 正向微分（Forward Differentiation）

假设有一个向量值函数 $\mathbf{y} = f(\mathbf{x})$，其中 $\mathbf{x} \in \mathbb{R}^n$ 是输入向量，$\mathbf{y} \in \mathbb{R}^m$ 是输出向量。函数 $f$ 的 Jacobian 矩阵 $\mathbf{J}$ 定义为：

$$
\mathbf{J} = \frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{bmatrix}
\frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_n} \\
\vdots & \ddots & \vdots \\
\frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_n}
\end{bmatrix}
$$

在正向微分中，我们关注于给定一个小的输入变化 $\Delta \mathbf{x}$，计算输出的变化 $\Delta \mathbf{y}$。使用 Jacobian 矩阵，我们可以通过下面的线性近似来估计输出的变化：

$$
\Delta \mathbf{y} \approx \mathbf{J} \Delta \mathbf{x}
$$

这里，$\Delta \mathbf{x}$ 是输入变化，$\Delta \mathbf{y}$ 是由此引起的输出变化的估计。

### 反向微分（Backward Differentiation 或自动微分）

反向微分，特别是在深度学习中常见的反向传播算法中，关注于给定输出变化对输入的影响。这里，我们通常关心的是损失函数 $L$ 对输入 $\mathbf{x}$ 的梯度 $\frac{\partial L}{\partial \mathbf{x}}$。

给定输出对损失的梯度 $\frac{\partial L}{\partial \mathbf{y}}$，我们可以使用链式法则和 Jacobian 矩阵来计算输入的梯度：

$$
\frac{\partial L}{\partial \mathbf{x}} = \mathbf{J}^T \frac{\partial L}{\partial \mathbf{y}}
$$

这里，$\mathbf{J}^T$ 是 Jacobian 矩阵的转置，$\frac{\partial L}{\partial \mathbf{y}}$ 是损失函数关于输出的梯度，$\frac{\partial L}{\partial \mathbf{x}}$ 是我们想要计算的损失函数关于输入的梯度。

---
思考：Jacobian 矩阵对于机器学习的意义是什么？
#card <!--2024/3/2/ZUMPK5a-->

它帮助计算损失函数关于每个参数的梯度。这些梯度随后被用来更新网络权重，以减少预测误差。

---
思考：为什么机器学习主要使用的是反向模式的自动微分？
#card <!--2024/3/2/lzGjEm6-->
因为当输入变量数量小于输出变量数量时，才使用前向模式的自动微分。

在深度学习中，输出变量（损失函数）通常远少于输入变量（模型参数），因此反向模式更快也更节省内存。

此外，反向模式自动微分天然支持动态计算图，这使得在模型结构动态变化时（如RNN中的序列长度变化），仍然可以高效地计算梯度。

---
思考：自动微分是否存在缺点？
#card <!--2024/3/2/Be3CM6T-->

- 存储开销：在**反向模式**自动微分中，为了计算梯度，需要存储前向传播的所有中间变量，这可能导致内存占用增加，尤其是在处理大型模型或数据集时。
- 性能开销：虽然自动微分通常很快，但对于某些**高度优化的计算**，手动实现的微分函数可能提供更好的性能。
- 灵活性限制：自动微分系统可能不支持某些**高级优化技术**，如手动实现的数值优化技巧，或者在一些复杂的优化问题中可能不如手动微分灵活。
---
自动微分有哪些方法？
#card <!--2024/3/2/pxSLg43-->
1. 基于表达式或者图（LIB）
	- 手动将表达式分解为库函数中的基本表达式函数的组合
	- 库函数定义了对应的表达式的微分规则和链式法则
2. 基于操作符重载（OO）
	- TensorFlow 和 PyTorch 都是用这种（混合第一种）。
	- 类似语法制导翻译，执行的时候顺带把微分给做了
3. 基于源码转换（AST）
	 - 从源码解析、转换，非常麻烦。
	 - 微分结果以代码形式存在，方便分布式计算，性能很高。