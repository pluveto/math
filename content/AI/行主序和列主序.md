当我们讨论坐标轴的顺序时，通常有两种常见的方式：列主序和行主序。在三维坐标系中，我们可以使用\(x\)、\(y\)和\(z\)来表示三个坐标轴。

列主序：
```
  z
  |
  |
  |____ y
 /
x


相当于

	 _______y
   /|
  / |
 z  x
```

行主序：
```
  z
 /
/_______x
|
|
y
```

这种看待坐标轴的差异决定了数据的布局。数学库一般都使用列主序。

行主序矩阵内存布局：

![](https://pic2.zhimg.com/80/v2-3d5cde0c897498f9a4bb0e71a060fb7d_1440w.webp)

列主序矩阵内存布局：

![](https://pic4.zhimg.com/80/v2-dc246d86c64f578189814522096201d3_1440w.webp)

## 加法算子的反向传播过程是怎样的

在深度学习中，加法算子的反向传播过程通常是比较简单的，因为加法本身是一个线性操作。假设我们有两个输入 $x$ 和 $y$，以及一个输出 $z = x + y$。

在反向传播过程中，我们需要计算相对于输入 $x$ 和 $y$ 的梯度，以便于进行梯度下降或者其他优化算法。由于加法是一个逐元素操作，其梯度的计算也是逐元素的。

假设我们有一个关于 $z$ 的损失函数 $L$，我们需要计算 $L$ 相对于 $x$ 和 $y$ 的梯度 $\frac{\partial L}{\partial x}$ 和 $\frac{\partial L}{\partial y}$。

根据链式法则：

$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial x}
$$
$$
\frac{\partial L}{\partial y} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial y}
$$

由于加法算子的梯度是1，因此 $\frac{\partial z}{\partial x} = 1$ 和 $\frac{\partial z}{\partial y} = 1$。

所以，我们可以得到：

$$
\frac{\partial L}{\partial x} = \frac{\partial L}{\partial z} \cdot 1 = \frac{\partial L}{\partial z}
$$
$$
\frac{\partial L}{\partial y} = \frac{\partial L}{\partial z} \cdot 1 = \frac{\partial L}{\partial z}
$$

换句话说，加法算子的反向传播过程就是将上游传来的梯度 $\frac{\partial L}{\partial z}$ 直接传递给每个输入 $x$ 和 $y$。